# Visualization Guide

This document provides a comprehensive overview of the visualizations generated by the meta-learning system.

## Visualization Directory Structure

By default, visualizations are saved to the following directories:

- Main visualizations: `results/visualizations/`
- Dynamic optimization visualizations: `results/` (direct)
- Optimizer comparisons: `results/optimizer_comparison/`
- Meta-learning results: `results/meta_learning/`
- Drift detection: `results/drift/`
- Optimization results: `results/optimization/`
- Algorithm selection demo: `results/algorithm_selection_demo/`
- Enhanced meta-learning: `results/enhanced_meta_main/`
- Performance comparisons: `results/performance/`

## Command Line Arguments for Visualizations

| Argument | Description | Visualization Files | Output Directory |
|----------|-------------|--------------------|------------------|
| `--visualize` | Enable general visualizations | All visualization types | `results/visualizations/` |
| `--meta` | Run meta-learning | Algorithm selection visualizations | `results/meta_learning/` |
| `--compare-optimizers` | Compare optimizers | Performance comparisons | `results/optimizer_comparison/` |
| `--boxplot` | Generate performance boxplots | Performance distribution boxplots | `results/performance/` |
| `--show-significance` | Show significance bands | Convergence curves with std dev bands | `results/` |
| `--drift` | Run drift detection | Drift analysis visualizations | `results/drift/` |
| `--explain` | Run explainability analysis | Feature importance, SHAP plots | `results/explainability/` |
| `--optimize` | Run optimization | Convergence plots | `results/optimization/` |
| `--dynamic-optimization` | Run dynamic optimization visualization | Dynamic optimization plots | `results/` |
| `--evaluate` | Evaluate ML model | Model evaluation plots | `results/` |
| `--test-algorithm-selection` | Run algorithm selection demo | Dashboard, timelines, frequency plots | `results/algorithm_selection_demo/` |
| `--interactive` | Generate interactive visualizations | Interactive HTML dashboards | `results/algorithm_selection_demo/` |
| `--enhanced-meta` | Run enhanced meta-learning | Enhanced meta visualizations | `results/enhanced_meta_main/` |
| `--visualize-drift` | Visualize concept drift | Drift plots for different functions | `results/` |
| `--dynamic` | Compare on dynamic problems | Dynamic radar charts | `results/` |

## Dynamic Optimization Visualizations

### Command Line Arguments

```bash
python main_v2.py --dynamic-optimization [OPTIONS]
```

| Option | Description | Default |
|--------|-------------|---------|
| `--function` | Test function to use ('ackley', 'rastrigin', etc.) | Required |
| `--drift-type` | Type of drift ('sudden', 'oscillatory', 'linear', etc.) | Required |
| `--dim` | Problem dimensionality | `10` |
| `--drift-rate` | Rate of drift (0.0 to 1.0) | `0.1` |
| `--drift-interval` | Interval between drift events | `20` |
| `--severity` | Severity of drift (0.0 to 1.0) | `1.0` |
| `--max-iterations` | Maximum number of iterations | `500` |
| `--reoptimize-interval` | Re-optimize after this many evaluations | `50` |
| `--export-dir` | Directory for saving visualizations | `results` |
| `--show-plot` | Show plot in addition to saving it | `False` |

### Visualization Output

The dynamic optimization visualization creates plots showing optimizer performance on dynamically changing problems:

1. **Dynamic Optimization Plot** (`dynamic_optimization_[function]_[drift_type].png`)
   - Shows how different optimizers perform on a function with concept drift
   - Tracks the optimal value as it changes due to drift
   - Displays optimization progress of different algorithms (DE, GWO, ACO, etc.)
   - Helps analyze optimizer robustness to changing environments

Example usage:
```bash
# Basic usage
python main_v2.py --dynamic-optimization --function=ackley --drift-type=sudden

# With custom parameters
python main_v2.py --dynamic-optimization --function=rastrigin --drift-type=oscillatory --dim=5 --drift-rate=0.2
```

The visualizations generated include:
- `dynamic_optimization_ackley_sudden.png`
- `dynamic_optimization_rastrigin_oscillatory.png`
- `dynamic_optimization_sphere_linear.png`
- etc.

## Algorithm Performance Visualizations

### Command Line Arguments

```bash
python main.py --compare-optimizers [OPTIONS]
```

| Option | Description | Default |
|--------|-------------|---------|
| `--dimension` | Dimension for optimization problems | `10` |
| `--visualize` | Enable visualization | Not enabled by default |
| `--export-dir` | Directory for exporting visualizations | `results` |
| `--boxplot` | Generate performance boxplots | Not enabled by default |
| `--show-significance` | Show significance bands on convergence plots | Not enabled by default |
| `--dynamic` | Compare optimizers on dynamic problems | Not enabled by default |

### Visualization Output

1. **Algorithm Radar Chart** (`algorithm_radar_chart.png`)
   - Shows how different algorithms perform across all problems
   - Each algorithm is represented as a polygon on the radar chart
   - Larger area indicates better overall performance

2. **Performance Heatmap** (`performance_heatmap.png`)
   - Shows the performance score of each algorithm on each problem
   - Color intensity represents performance (darker colors typically indicate better performance)
   - Allows quick identification of which algorithms perform well on specific problems

3. **Performance Boxplot** (`performance_boxplot.png`)
   - Shows the distribution of performance metrics across algorithms
   - Includes median, quartiles, and outliers for each algorithm
   - Helps identify not just average performance but also consistency
   - Generated using `--compare-optimizers --boxplot`

4. **Performance Profile** (`performance_profile.png`)
   - Shows the fraction of problems for which each algorithm is within a factor τ of the best algorithm
   - The higher the curve, the more robust the algorithm is across different problems
   - Algorithms with curves that start higher have better performance on more problems

5. **Algorithm Ranking** (`algorithm_ranks.png`)
   - Displays the average rank of each algorithm across all problems
   - Lower rank means better performance
   - Provides a simple way to compare overall algorithm effectiveness

6. **Convergence Plots with Significance** (`convergence_[problem_name].png`)
   - Shows how each algorithm's performance improves over iterations for a specific problem
   - Includes standard deviation bands around the mean performance
   - Helps visualize not just convergence but also reliability/consistency
   - Generated using `--compare-optimizers --show-significance`
   - Steeper slopes indicate faster convergence
   - Lower final values indicate better solutions

7. **Dynamic Radar Charts** (`radar_[function]_dynamic_[drift_type].png`)
   - Compares optimizer performance on dynamic problems
   - Shows how algorithms perform under different drift conditions
   - Each axis represents a performance metric
   - Generated using `--compare-optimizers --dynamic`

8. **Comparative Convergence** (`comparative_convergence.png`)
   - Normalizes convergence across different problems for comparison
   - Shows which algorithms generally converge faster
   - Helps identify algorithms that make rapid initial progress vs. those that find better final solutions

## Meta-Learning Visualizations

### Command Line Arguments

```bash
python main.py --meta [OPTIONS]
```

| Option | Description | Default |
|--------|-------------|---------|
| `--method` | Method for meta-learner | `bayesian` |
| `--visualize` | Enable visualization | Not enabled by default |
| `--dimension` | Dimension for optimization problems | `10` |

### Visualization Output

1. **Selection Frequency Heatmap** (`selection_frequency_heatmap.png`)
   - Shows how often each algorithm is selected for problems with specific features
   - Helps identify patterns in algorithm selection based on problem characteristics

2. **Algorithm Selection Frequency** (`algorithm_selection_frequency.png`)
   - Displays the overall frequency of algorithm selection across all problems
   - Shows which algorithms are chosen most often by the meta-learning system

3. **Algorithm Selection Dashboard** (`algorithm_selection_dashboard.png`)
   - Comprehensive dashboard with multiple visualizations
   - Includes selection frequency, problem type distribution, timeline of selections
   - Shows performance comparison across algorithms and problem types
   - Provides detailed statistics table for algorithm performance

4. **Feature Analysis** (`feature_correlation_clustered.png`, `feature_pca_projection.png`)
   - Shows correlations between different problem features
   - Visualizes problem features in a 2D space using Principal Component Analysis
   - Similar problems appear closer together
   - Feature vectors show the direction and strength of each feature's influence

5. **Problem Clustering** (`problem_hierarchical_clustering.png`)
   - Groups similar problems based on their features
   - Hierarchical clustering shows relationships between problem groups

6. **Feature Importance** (`feature_importance.png`)
   - Shows which features have the greatest impact on algorithm selection
   - Higher values indicate features that strongly influence which algorithm works best

7. **Feature-Algorithm Correlation** (`feature_algorithm_correlation.png`)
   - Shows relationships between specific features and algorithm performance
   - Helps understand which features favor certain algorithms

## Drift Detection Visualizations

### Command Line Arguments

```bash
python main.py --drift [OPTIONS]
```

| Option | Description | Default |
|--------|-------------|---------|
| `--drift-window` | Window size for drift detection | `50` |
| `--drift-threshold` | Threshold for drift detection | `0.5` |
| `--drift-significance` | Significance level for drift detection | `0.05` |
| `--visualize` | Enable visualization | Not enabled by default |

### Visualization Output

1. **Drift Detection Results** (`drift_detection_results.png`)
   - Shows signal data with detected drift points
   - Displays drift severity over time
   - Visualizes trends that may indicate potential drift
   - Helps identify when and where data drift occurs

2. **Feature Drift Analysis** (`feature_drift_analysis.png`)
   - Shows feature value changes with marked drift points
   - Compares drift scores and p-values against significance thresholds
   - Displays feature contributions to detected drift
   - Helps understand which features are causing drift

## Explainability Visualizations

### Command Line Arguments

```bash
python main.py --explain [OPTIONS]
```

| Option | Description | Default |
|--------|-------------|---------|
| `--explainer` | Explainer type to use (`shap`, `lime`, etc.) | `shap` |
| `--explain-plots` | Generate and save explainability plots | `False` |
| `--explain-samples` | Number of samples to use for explainability | `5` |
| `--explain-plot-types` | Specific plot types to generate | None |

### Visualization Output

1. **Feature Importance** (`feature_importance.png`)
   - General feature importance across all samples
   - Shows which features most influence model predictions

2. **Feature Importance by Algorithm** (`feature_importance_[algorithm].png`)
   - Feature importance for specific algorithms (e.g., `feature_importance_DE.png`)
   - Shows which features are most important for each algorithm

3. **SHAP Summary Plot** (when using SHAP explainer)
   - Shows how each feature affects the model output
   - Color indicates feature value (red high, blue low)
   - Position on x-axis shows impact on model output

4. **SHAP Dependence Plots** (when using SHAP explainer)
   - Shows how changes in feature value affect predictions
   - Can reveal non-linear relationships and interactions

5. **LIME Explanations** (when using LIME explainer)
   - Local explanations for individual predictions
   - Shows which features contribute to specific predictions

## Pipeline Performance Visualizations

### Command Line Arguments

```bash
python main.py --pipeline-monitoring [OPTIONS]
```

| Option | Description | Default |
|--------|-------------|---------|
| `--monitor-period` | Monitoring period in days | `30` |
| `--visualize` | Enable visualization | Not enabled by default |

### Visualization Output

1. **Pipeline Performance Visualization** (`pipeline_performance.png`)
   - Tracks drift scores over time with threshold indicators
   - Shows severity of individual feature drifts
   - Tracks model confidence over time against threshold
   - Helps monitor the health of machine learning pipelines in production

## Interpreting Visualizations

When analyzing these visualizations, look for:

1. **Patterns**: Do certain algorithms consistently outperform others on specific types of problems?
2. **Correlations**: Which problem features correlate with algorithm performance?
3. **Clusters**: Are there natural groupings of problems that respond similarly to algorithms?
4. **Outliers**: Are there problems where the performance pattern differs significantly from others?
5. **Drift Patterns**: Are there consistent patterns in when and how data drift occurs?
6. **Feature Contributions**: Which features contribute most to data drift or algorithm selection?
7. **Dynamic Performance**: How do algorithms perform when the problem changes over time?
8. **Robustness to Drift**: Which algorithms recover quickly after drift occurs?

These insights can guide:
- Which algorithm to choose for a new problem
- How to characterize problems to predict algorithm performance
- Where to focus efforts for improving algorithm selection
- When to retrain models due to data drift
- Which features to monitor most closely
- Which algorithms are most robust in changing environments

## Customizing Visualizations

You can customize the appearance of these visualizations by modifying the corresponding functions:

- For general visualizations: `core/meta_learning.py`
- For dynamic optimization visualizations: `visualization/dynamic_optimization_viz.py`
- For drift detection visualizations: `drift/detector.py`
- For explainability visualizations: `explainability/explainer.py`

Common customization options include changing color maps, figure sizes, and annotation formats.

## Testing Visualizations

A comprehensive test script is provided to generate sample visualizations using test data:

```bash
python test_all_visualizations.py
```

For dynamic optimization visualizations, you can test with:

```bash
# Test different function and drift type combinations
python main_v2.py --dynamic-optimization --function=ackley --drift-type=sudden
python main_v2.py --dynamic-optimization --function=rastrigin --drift-type=oscillatory
python main_v2.py --dynamic-optimization --function=sphere --drift-type=linear
```

## Summary of Available Visualizations

| Visualization Type | Command | Output File |
|--------------------|---------|-------------|
| **Dynamic Optimization** | `--dynamic-optimization` | `dynamic_optimization_[function]_[drift_type].png` |
| **Algorithm Radar Chart** | `--compare-optimizers` | `algorithm_radar_chart.png` |
| **Performance Heatmap** | `--compare-optimizers` | `performance_heatmap.png` |
| **Performance Boxplot** | `--compare-optimizers --boxplot` | `performance_boxplot.png` |
| **Performance Profile** | `--compare-optimizers` | `performance_profile.png` |
| **Algorithm Ranking** | `--compare-optimizers` | `algorithm_ranks.png` |
| **Convergence Plots** | `--optimize` or `--compare-optimizers` | `convergence_[problem_name].png` |
| **Convergence with Significance** | `--compare-optimizers --show-significance` | `convergence_[problem_name].png` |
| **Dynamic Radar Charts** | `--compare-optimizers --dynamic` | `radar_[function]_dynamic_[drift_type].png` |
| **Comparative Convergence** | `--compare-optimizers` | `comparative_convergence.png` |
| **Selection Frequency** | `--meta` | `selection_frequency_heatmap.png`, `algorithm_selection_frequency.png` |
| **Algorithm Dashboard** | `--test-algorithm-selection` | `algorithm_selection_dashboard.png` |
| **Algorithm Selection Timeline** | `--test-algorithm-selection` | `algorithm_selection_timeline.png` |
| **Algorithm Selection by Problem** | `--test-algorithm-selection` | `algorithm_selection_by_problem.png` |
| **Algorithm Selection by Phase** | `--test-algorithm-selection` | `algorithm_selection_by_phase.png` |
| **Interactive Dashboards** | `--test-algorithm-selection --interactive` | `interactive_dashboard.html`, `interactive_algorithm_timeline.html` |
| **Model Evaluation** | `--evaluate --visualize` | `model_evaluation.png` |
| **Drift Visualization** | `--drift --visualize-drift` | `drift_[function]_[drift_type].png` |
| **Feature Analysis** | `--meta` | `feature_correlation_clustered.png`, `feature_pca_projection.png` |
| **Problem Clustering** | `--meta` | `problem_hierarchical_clustering.png` |
| **Feature Importance** | `--meta` or `--explain` | `feature_importance.png` |
| **Feature-Algorithm Correlation** | `--meta` | `feature_algorithm_correlation.png` |
| **Drift Detection** | `--drift` | `drift_detection_results.png`, `feature_drift_analysis.png` |
| **Pipeline Performance** | `--pipeline-monitoring` | `pipeline_performance.png` |
| **SHAP Plots** | `--explain` with `--explainer shap` | Various SHAP output files |
| **LIME Plots** | `--explain` with `--explainer lime` | Various LIME output files |

## Algorithm Selection Demo Visualizations

### Command Line Arguments

```bash
python main.py --test-algorithm-selection [OPTIONS]
```

| Option | Description | Default |
|--------|-------------|---------|
| `--algo-viz-dir` | Directory to save visualizations | `results/algorithm_selection_demo` |
| `--interactive` | Generate interactive HTML visualizations | Not enabled by default |

### Visualization Output

1. **Algorithm Selection Dashboard** (`algorithm_selection_dashboard.png`)
   - Comprehensive dashboard with multiple visualizations
   - Includes selection frequency, problem type distribution, timeline of selections
   - Shows performance comparison across algorithms and problem types
   - Provides detailed statistics table for algorithm performance

2. **Algorithm Selection Frequency** (`algorithm_selection_frequency.png`)
   - Bar chart showing how often each algorithm is selected
   - Helps identify which algorithms are most commonly chosen by the system

3. **Algorithm Selection Timeline** (`algorithm_selection_timeline.png`)
   - Shows how algorithm selection changes over time or iterations
   - Useful for identifying shifts in algorithm preference

4. **Algorithm Selection by Problem** (`algorithm_selection_by_problem.png`)
   - Heatmap showing which algorithms are selected for which problems
   - Helps identify patterns in algorithm-problem matching

5. **Algorithm Selection by Phase** (`algorithm_selection_by_phase.png`)
   - Shows which algorithms are selected during different optimization phases
   - Helps understand the meta-learner's phase-based selection strategy

6. **Optimizer Performance Comparison** (`optimizer_performance_comparison.png`)
   - Direct comparison of performance metrics across optimizers
   - Helps validate the algorithm selection decisions

7. **Interactive Dashboard** (`interactive_dashboard.html`)
   - Interactive HTML version of the dashboard with tooltips and filters
   - Allows deeper exploration of algorithm selection patterns
   - Generated using `--test-algorithm-selection --interactive`

8. **Interactive Algorithm Timeline** (`interactive_algorithm_timeline.html`)
   - Interactive timeline showing algorithm selection changes
   - Allows zooming and filtering by time periods
   - Generated using `--test-algorithm-selection --interactive`

## Enhanced Meta-Learning Visualizations

### Command Line Arguments

```bash
python main.py --enhanced-meta [OPTIONS]
```

| Option | Description | Default |
|--------|-------------|---------|
| `--export-dir` | Directory for exporting visualizations | `results/enhanced_meta_main` |
| `--visualize` | Enable visualization | Not enabled by default |

### Visualization Output

Enhanced meta-learning generates various JSON files with results and creates visualizations in the specified directory:

1. **Problem Features** (`problem_features.json`)
   - Contains extracted features for each optimization problem
   - Used for meta-learning and visualization

2. **Enhanced Meta Results** (`enhanced_meta_results.json`)
   - Detailed results from the enhanced meta-learning process
   - Includes performance metrics and algorithm selections

3. **Enhanced Meta Selections** (`enhanced_meta_selections.json`)
   - Algorithm selections made by the enhanced meta-learner
   - Includes reasoning and confidence scores

4. **Visualizations Subfolder** (`visualizations/`)
   - Contains generated visualizations based on the enhanced meta-learning results
   - May include custom visualizations specific to the enhanced meta-learning process

## Model Evaluation Visualizations

### Command Line Arguments

```bash
python main.py --evaluate --visualize [OPTIONS]
```

| Option | Description | Default |
|--------|-------------|---------|
| `--export-dir` | Directory for exporting visualizations | `results` |

### Visualization Output

1. **Model Evaluation Plot** (`model_evaluation.png`)
   - Scatter plot of actual vs. predicted values
   - Includes performance metrics: MSE, RMSE, R²
   - Helps assess model accuracy and bias
   - Shows the distribution of predictions compared to true values
   - Generated using `--evaluate --visualize`

## Drift Visualization

### Command Line Arguments

```bash
python main.py --drift --visualize-drift [OPTIONS]
```

| Option | Description | Default |
|--------|-------------|---------|
| `--function` | Test function to use | Required |
| `--drift-type` | Type of drift | Required |
| `--export-dir` | Directory for exporting visualizations | `results` |

### Visualization Output

1. **Drift Visualization Plots** (`drift_[function]_[drift_type].png`)
   - Shows function evaluations over time with drift
   - Visualizes how the optimal value changes due to concept drift
   - Helps understand the nature and impact of different drift types
   - Examples: `drift_levy_linear.png`, `drift_rastrigin_oscillatory.png`
   - Generated using `--drift --visualize-drift --function=[function] --drift-type=[type]` 