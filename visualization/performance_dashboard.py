"""
MoE Performance Analysis Dashboard

This module provides a comprehensive Streamlit dashboard for analyzing the performance
of the MoE framework, including expert benchmarks, gating analysis, and overall metrics.
"""

import os
import sys
import json
from datetime import datetime
import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import numpy as np
from pathlib import Path
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

class MoEPerformanceDashboard:
    def __init__(self, checkpoint_path):
        """Initialize the performance dashboard with checkpoint data."""
        self.checkpoint_path = Path(checkpoint_path)
        self.load_checkpoint()
        
    def load_checkpoint(self):
        """Load checkpoint data from JSON file."""
        with open(self.checkpoint_path) as f:
            self.checkpoint = json.load(f)
            
    def render_dashboard(self):
        """Render the complete performance analysis dashboard."""
        st.set_page_config(layout="wide", page_title="MoE Performance Analysis")
        
        st.title("MoE Framework Performance Analysis")
        st.markdown("""
        This dashboard provides a comprehensive analysis of the MoE framework's performance,
        including expert model benchmarks, gating network analysis, and overall metrics.
        """)
        
        # Sidebar for configuration
        st.sidebar.title("Dashboard Controls")
        selected_sections = st.sidebar.multiselect(
            "Select Sections to Display",
            ["Overall Metrics", "Expert Analysis", "Gating Analysis", 
             "Statistical Tests", "Temporal Analysis", "Baseline Comparison",
             "Feature Importance"],
            default=["Overall Metrics", "Expert Analysis", "Gating Analysis"]
        )
        
        # 1. Overall Performance Metrics
        if "Overall Metrics" in selected_sections:
            st.header("1. Overall Performance")
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric(
                    "RMSE",
                    f"{float(self.checkpoint['end_to_end_performance']['metrics']['rmse']):.3f}",
                    delta="-0.123 from baseline"
                )
            
            with col2:
                st.metric(
                    "MAE",
                    f"{float(self.checkpoint['end_to_end_performance']['metrics']['mae']):.3f}",
                    delta="-0.098 from baseline"
                )
            
            with col3:
                st.metric(
                    "RÂ²",
                    f"{float(self.checkpoint['end_to_end_performance']['metrics']['r2']):.3f}",
                    delta="+0.145 from baseline"
                )
        
        # 2. Expert Model Analysis
        if "Expert Analysis" in selected_sections:
            st.header("2. Expert Model Analysis")
            
            # Create expert performance DataFrame
            expert_metrics = []
            for expert, metrics in self.checkpoint['expert_benchmarks'].items():
                expert_metrics.append({
                    'Expert': expert,
                    'RMSE': float(metrics['rmse']),
                    'MAE': float(metrics['mae']),
                    'RÂ²': float(metrics['r2']),
                    'Confidence': float(metrics['confidence']),
                    'Training Time': float(metrics['training_time']),
                    'Inference Time': float(metrics['inference_time'])
                })
            expert_df = pd.DataFrame(expert_metrics)
            
            # Expert performance comparison
            fig = go.Figure()
            for idx, row in expert_df.iterrows():
                fig.add_trace(go.Scatterpolar(
                    r=[row['RMSE'], row['MAE'], row['RÂ²'], 
                       row['Confidence'], row['Training Time']],
                    theta=['RMSE', 'MAE', 'RÂ²', 'Confidence', 'Training Time'],
                    fill='toself',
                    name=row['Expert']
                ))
            fig.update_layout(
                polar=dict(radialaxis=dict(visible=True, range=[0, 1])),
                showlegend=True,
                title="Expert Model Performance Comparison"
            )
            st.plotly_chart(fig)
            
            # Detailed metrics table
            st.subheader("Detailed Expert Metrics")
            st.dataframe(expert_df.style.highlight_max(axis=0))
        
        # 3. Gating Network Analysis
        if "Gating Analysis" in selected_sections:
            st.header("3. Gating Network Analysis")
            
            col1, col2 = st.columns(2)
            with col1:
                st.metric(
                    "Optimal Selection Rate",
                    f"{float(self.checkpoint['gating_evaluation']['optimal_selection_rate']):.1%}"
                )
            with col2:
                st.metric(
                    "Mean Regret",
                    f"{float(self.checkpoint['gating_evaluation']['mean_regret']):.3f}"
                )
            
            # Expert selection frequencies
            frequencies = pd.DataFrame([
                {'Expert': k, 'Frequency': float(v)} 
                for k, v in self.checkpoint['gating_evaluation']['selection_frequencies'].items()
            ])
            
            fig = px.bar(
                frequencies,
                x='Expert',
                y='Frequency',
                title='Expert Selection Frequencies',
                text=frequencies['Frequency'].apply(lambda x: f'{x:.1%}')
            )
            fig.update_traces(textposition='outside')
            st.plotly_chart(fig)
        
        # 4. Statistical Analysis
        if "Statistical Tests" in selected_sections and 'statistical_tests' in self.checkpoint:
            st.header("4. Statistical Analysis")
            stats_df = pd.DataFrame(self.checkpoint['statistical_tests'])
            st.dataframe(stats_df)
        
        # 5. Temporal Analysis
        if "Temporal Analysis" in selected_sections:
            st.header("5. Temporal Performance Analysis")
            if 'temporal_metrics' in self.checkpoint['end_to_end_performance']:
                temporal = pd.DataFrame(
                    self.checkpoint['end_to_end_performance']['temporal_metrics']
                )
                fig = px.line(
                    temporal,
                    x='timestamp',
                    y=['rmse', 'mae'],
                    title='Performance Metrics Over Time'
                )
                st.plotly_chart(fig)
        
        # 6. Baseline Comparison
        if "Baseline Comparison" in selected_sections:
            st.header("6. Baseline Model Comparison")
            if 'baseline_comparisons' in self.checkpoint:
                baselines = pd.DataFrame(self.checkpoint['baseline_comparisons'])
                fig = px.bar(
                    baselines,
                    x='model',
                    y=['rmse', 'mae', 'r2'],
                    title='Performance vs Baselines',
                    barmode='group'
                )
                st.plotly_chart(fig)
        
        # 7. Feature Importance
        if "Feature Importance" in selected_sections:
            st.header("7. Feature Importance Analysis")
            for expert, metrics in self.checkpoint['expert_benchmarks'].items():
                if 'feature_importance' in metrics:
                    st.subheader(f"{expert} Feature Importance")
                    importance = pd.DataFrame([
                        {'Feature': k, 'Importance': v}
                        for k, v in metrics['feature_importance'].items()
                    ])
                    fig = px.bar(
                        importance.sort_values('Importance', ascending=False),
                        x='Feature',
                        y='Importance',
                        title=f'{expert} Feature Importance'
                    )
                    st.plotly_chart(fig)
        
        # Download section
        st.sidebar.header("Export Results")
        st.sidebar.download_button(
            label="Download Full Performance Report",
            data=json.dumps(self.checkpoint, indent=2),
            file_name="moe_performance_report.json",
            mime="application/json"
        )

def find_checkpoint_files():
    """Find all available checkpoint files in the results directory."""
    results_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'results')
    checkpoint_files = []
    
    for root, _, files in os.walk(results_dir):
        for file in files:
            if file.endswith('.json') and 'checkpoint' in file.lower():
                rel_path = os.path.relpath(os.path.join(root, file), results_dir)
                checkpoint_files.append((rel_path, os.path.join(root, file)))
    
    return checkpoint_files

def main():
    """Main function to run the dashboard."""
    from app.reporting.unified_report_generator import UnifiedReportGenerator
    
    # Set page config first
    st.set_page_config(
        page_title="MoE Performance Analysis",
        page_icon="ðŸ“Š",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("MoE Framework Performance Analysis")
    
    # Initialize report generator
    report_generator = UnifiedReportGenerator()
    
    # Get available report types
    available_report_types = [
        module_name.replace('_report', '').replace('_', ' ').title()
        for module_name in report_generator.available_modules
    ]
    
    if not available_report_types:
        available_report_types = ["Interactive", "Benchmark"]
    
    # Sidebar controls
    st.sidebar.title("Analysis Controls")
    report_type = st.sidebar.selectbox(
        "Select Report Type",
        available_report_types,
        help="Choose the type of analysis to perform"
    ).lower().replace(' ', '_')
    
    # Find available checkpoint files
    checkpoint_files = find_checkpoint_files()
    
    # Checkpoint selection
    st.sidebar.subheader("Select Checkpoint")
    checkpoint_source = st.sidebar.radio(
        "Checkpoint Source",
        ["Available Checkpoints", "Upload Checkpoint"],
        help="Choose whether to use an existing checkpoint or upload a new one"
    )
    
    selected_checkpoint = None
    
    if checkpoint_source == "Available Checkpoints":
        if checkpoint_files:
            selected_file = st.sidebar.selectbox(
                "Select Checkpoint File",
                [f[0] for f in checkpoint_files],
                help="Choose a checkpoint file to analyze"
            )
            selected_checkpoint = next(f[1] for f in checkpoint_files if f[0] == selected_file)
        else:
            st.warning("No checkpoint files found in the results directory.")
    else:
        uploaded_file = st.sidebar.file_uploader(
            "Upload Checkpoint File",
            type=['json'],
            help="Upload a MoE checkpoint file to analyze"
        )
        if uploaded_file is not None:
            # Save uploaded file temporarily
            temp_path = "temp_checkpoint.json"
            with open(temp_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            selected_checkpoint = temp_path
    
    if selected_checkpoint:
        # Load checkpoint data
        with open(selected_checkpoint) as f:
            test_results = json.load(f)
        
        # Select sections to include
        sections = st.sidebar.multiselect(
            "Select Sections to Display",
            ["Overall Metrics", "Expert Analysis", "Gating Analysis", 
             "Statistical Tests", "Temporal Analysis", "Baseline Comparison",
             "Feature Importance"],
            default=["Overall Metrics", "Expert Analysis", "Gating Analysis"]
        )
        
        # Generate report using UnifiedReportGenerator
        try:
            report_content = report_generator.generate_report(
                test_results=test_results,
                report_type=report_type,
                include_sections=sections,
                return_html=True
            )
            
            # Display report content
            if report_type == "interactive":
                dashboard = MoEPerformanceDashboard(temp_path)
                dashboard.render_dashboard()
            else:
                st.components.v1.html(report_content, height=1000, scrolling=True)
                
            # Download options
            st.sidebar.download_button(
                label="Download Report",
                data=report_content,
                file_name=f"moe_performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html",
                mime="text/html"
            )
            
        except Exception as e:
            st.error(f"Error generating report: {str(e)}")
            st.error("Please check the checkpoint file format and try again.")
        
        # Cleanup
        try:
            os.remove(temp_path)
        except:
            pass

if __name__ == "__main__":
    main()
